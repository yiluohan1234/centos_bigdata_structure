#!/bin/bash
INSTALL_PATH=/opt/module

check_process(){
    pid=$(ps -ef 2>/dev/null | grep -v grep | grep -i $1 | awk '{print $2}')
    ppid=$(netstat -nltp 2>/dev/null | grep $2 | awk '{print $7}' | cut -d '/' -f 1)
    echo $pid
    [[ "$pid" =~ "$ppid" ]] && [ "$ppid" ] && return 0 || return 1
}

dfs(){
    usage="Usage: $0 (start|stop)"

    if [ $# -lt 1 ]; then
        echo $usage
        exit 1
    fi
    case $1 in
        start)
            $INSTALL_PATH/hadoop/sbin/start-dfs.sh
            ;;
        stop)
            $INSTALL_PATH/hadoop/sbin/stop-dfs.sh
            ;;
        format)
            $INSTALL_PATH/hadoop/bin/hdfs namenode -format
            ;;
        restart)
            dfs stop
            dfs start
            ;;
        *)
            echo $usage
            ;;
    esac
}

yarn(){
    usage="Usage: $0 (start|stop)"

    if [ $# -lt 1 ]; then
        echo $usage
        exit 1
    fi
    case $1 in
        start)
            $INSTALL_PATH/hadoop/sbin/start-yarn.sh
            ;;
        stop)
            $INSTALL_PATH/hadoop/sbin/stop-yarn.sh
            ;;
        restart)
            yarn stop
	        yarn start
            ;;
        *)
            echo $usage
            ;;
    esac
}

historyserver(){
    usage="Usage: $0 (start|stop)"

    if [ $# -lt 1 ]; then
        echo $usage
        exit 1
    fi
    case $1 in
        start)
            echo " --------------- 启动 historyserver ---------------"
            $INSTALL_PATH/hadoop/sbin/mr-jobhistory-daemon.sh start historyserver
            ;;
        stop)
            echo " --------------- 关闭 historyserver ---------------"
            $INSTALL_PATH/hadoop/sbin/mr-jobhistory-daemon.sh stop historyserver
            ;;
        restart)
            historyserver stop
	        historyserver start
            ;;
        *)
            echo $usage
            ;;
    esac
}

hadoop(){
    usage="Usage: $0 (start|stop|format)"

    if [ $# -lt 1 ]; then
        echo $usage
        exit 1
    fi
    case $1 in
        start)
            $INSTALL_PATH/hadoop/sbin/start-all.sh
            ;;
        stop)
            $INSTALL_PATH/hadoop/sbin/stop-all.sh
            ;;
        restart)
            hadoop stop
            hadoop start
            ;;
        format)
            ssh hadoop "$INSTALL_PATH/hadoop/bin/hdfs namenode -format"
            ;;
        *)
            echo $usage
            ;;
    esac
}

zk(){
    usage="Usage: $0 (start|stop|status)"

    if [ $# -lt 1 ]; then
        echo $usage
        exit 1
    fi
    case $1 in
        start)
            $INSTALL_PATH/zookeeper/bin/zkServer.sh start
            ;;
        stop)
            INSTALL_PATH/zookeeper/bin/zkServer.sh stop
            ;;
        status)
            $INSTALL_PATH/zookeeper/bin/zkServer.sh status
            ;;
        *)
            echo $usage
            ;;
    esac
}
#一键启动集群
kafka(){
    usage="Usage: $0 (start|stop)"

    if [ $# -lt 1 ]; then
        echo $usage
        exit 1
    fi
    case $1 in
        start)
            $INSTALL_PATH/kafka/bin/kafka-server-start.sh -daemon $INSTALL_PATH/kafka/config/server.properties
            ;;
        stop)
            ps -ef | awk '/Kafka/ && !/awk/{print $2}' | xargs kill -9
            ;;
        *)
            echo $usage
            ;;
    esac
}

spark(){
    usage="Usage(spark): $0 (start|stop)"

    if [ $# -lt 1 ]; then
        echo $usage
        exit 1
    fi
    case $1 in
        start)
            $INSTALL_PATH/spark/sbin/start-all.sh
            ;;
        stop)
            $INSTALL_PATH/spark/sbin/stop-all.sh
            ;;
        *)
            echo $usage
            ;;
    esac
}

flink(){
    usage="Usage(flink): $0 (start|stop)"

    if [ $# -lt 1 ]; then
        echo $usage
        exit 1
    fi
    case $1 in
        start)
            $INSTALL_PATH/flink/bin/start-cluster.sh
            ;;
        stop)
            $INSTALL_PATH/flink/bin/stop-cluster.sh
            ;;
        *)
            echo $usage
            ;;
    esac
}

hbase(){
    usage="Usage(hbase): $0 (start|stop)"

    if [ $# -lt 1 ]; then
        echo $usage
        exit 1
    fi
    case $1 in
        start)
            $INSTALL_PATH/hbase/bin/start-hbase.sh
            ;;
        stop)
            $INSTALL_PATH/hbase/bin/stop-hbase.sh
            ;;
        *)
            echo $usage
            ;;
    esac
}

hive(){
    USAGE="Usage: $0 (start|stop|status)"
    HIVE_LOG_DIR=$INSTALL_PATH/hive/logs
    [ ! -d ${HIVE_LOG_DIR} ] && mkdir ${HIVE_LOG_DIR}

    case $1 in
        start)
            metapid=$(check_process HiveMetastore 9083)
            cmd="nohup hive --service metastore > $HIVE_LOG_DIR/metastore.log 2>&1 &"
            cmd=$cmd" sleep 4; hdfs dfsadmin -safemode wait >/dev/null 2>&1"
            [ -z "$metapid" ] && eval $cmd || echo "Metastroe服务已启动"
            server2pid=$(check_process HiveServer2 10000)
            cmd="nohup hive --service hiveserver2 > $HIVE_LOG_DIR/hiveServer2.log 2>&1 &"
            [ -z "$server2pid" ] && eval $cmd || echo "HiveServer2服务已启动"
            ;;
        stop)
            metapid=$(check_process HiveMetastore 9083)
            [ "$metapid" ] && kill $metapid || echo "Metastore服务未启动"
            server2pid=$(check_process HiveServer2 10000)
            [ "$server2pid" ] && kill $server2pid || echo "HiveServer2服务未启动"
            ;;
        status)
            check_process HiveMetastore 9083 >/dev/null && echo "Metastore服务运行正常" || echo "Metastore服务运行异常"
            check_process HiveServer2 10000 >/dev/null && echo "HiveServer2服务运行正常" || echo "HiveServer2服务运行异常"
            ;;
        "init")
            schematool -initSchema -dbType mysql
            ;;
        "restart")
            hive stop
            sleep 2
            hive start
            ;;
        *)
            echo $USAGE
            exit 1
            ;;
    esac
}

args()
{
    usage="Usage: $0 (dfs|yarn|zk|kafka|spark|flink|hbase|start|stop)"

    if [ $# -lt 2 ]; then
        echo $usage
        exit 1
    fi

    case $1 in
	  dfs)
		dfs $2
		;;
	  yarn)
		yarn $2
		;;
	  historyserver)
		historyserver $2
		;;
	  hdp)
		hadoop $2
		;;
	  spark)
		spark $2
		;;
	  zookeeper)
		zk $2
		;;
	  flink)
		flink $2
		;;
	  hbase)
		hbase $2
		;;
	  kafka)
		kafka $2
		;;
	  hive)
		hive $2
		;;
	  *)
		echo $usage
		;;
    esac
}
args $@
