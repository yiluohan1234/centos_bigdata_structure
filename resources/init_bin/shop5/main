#!/bin/bash
# log: f1->f2->lg(2020-06-14)
# db: gen_import_config.sh->mysql_to_hdfs_full.sh all 2020-06-14->f3->db->清除maxwel断点，maxwell修改2020-06-14->mysql_to_kafka_inc_init.sh all
# drop table maxwell.bootstrap;drop table maxwell.columns;drop table maxwell.databases;drop table maxwell.heartbeats;drop table maxwell.positions;drop table maxwell.schemas;drop table maxwell.tables;


INSTALL_PATH=/opt/module
f1(){
    case $1 in
        start)
            for i in hdp101 hdp102
            do
                echo " --------启动 $i 采集flume-------"
                ssh $i "source /etc/profile; nohup /opt/module/flume/bin/flume-ng agent -n a1 -c /opt/module/flume/conf/ -f /opt/module/flume/job/file_to_kafka.conf >/dev/null 2>&1 &"
            done
            ;;
        stop)
            for i in hdp101 hdp102
            do
                echo " --------停止 $i 采集flume-------"
                ssh $i "ps -ef | grep file_to_kafka | grep -v grep |awk  '{print \$2}' | xargs -n1 kill -9 "
            done
            ;;
    esac
}
f2(){
    case $1 in
        start)
            echo " --------启动 hdp103 日志数据flume-------"
            ssh hdp103 "source /etc/profile; nohup /opt/module/flume/bin/flume-ng agent -n a1 -c /opt/module/flume/conf -f /opt/module/flume/job/kafka_to_hdfs_log.conf >/dev/null 2>&1 &"
        ;;
        stop)
            echo " --------停止 hdp103 日志数据flume-------"
            ssh hdp103 "ps -ef | grep kafka_to_hdfs_log | grep -v grep |awk '{print \$2}' | xargs -n1 kill"
        ;;
    esac
}
f3() {
    case $1 in
        start)
            echo " --------启动 hdp103 业务数据flume-------"
            ssh hdp103 "source /etc/profile; nohup /opt/module/flume/bin/flume-ng agent -n a1 -c /opt/module/flume/conf -f /opt/module/flume/job/kafka_to_hdfs_db.conf >/dev/null 2>&1 &"
        ;;
        stop)

            echo " --------停止 hdp103 业务数据flume-------"
            ssh hdp103 "ps -ef | grep kafka_to_hdfs_db | grep -v grep |awk '{print \$2}' | xargs -n1 kill"
        ;;
    esac
}

lg() {
    for i in hdp101 hdp102; do
        echo "========== $i =========="
        ssh $i "source /etc/profile; cd ${INSTALL_PATH}/dataware/log; java -jar gmall2020-mock-log-2021-10-10.jar >/dev/null 2>&1 &"
    done
}

lg_init(){
    local date=`date -d "$1" +"%Y-%m-%d"`
    echo "============ 生成 $date log数据 ============"
    sed -i "s@^mock.date:.*@mock.date: \"$date\"@g" ${INSTALL_PATH}/dataware/log/application.yml
    ssh hdp101 "source /etc/profile; cd ${INSTALL_PATH}/dataware/log; java -jar gmall2020-mock-log-2021-10-10.jar >/dev/null 2>&1 &"
}

db() {
    ssh hdp101 "source /etc/profile; cd ${INSTALL_PATH}/dataware/db; java -jar gmall2020-mock-db-2021-11-14.jar >/dev/null 2>&1 &"
}

db_init(){
    local start_date=`date -d "$1" +"%Y%m%d"`
    local last_date=`date -d "$2" +"%Y%m%d"`
    local cur_date=$start_date

    date_list=""
    while [ $cur_date -le $last_date ];
    do
      date_list="$date_list $cur_date"
      cur_date=`date -d "$cur_date +1 day" +"%Y%m%d"`
    done

    for dy in ${date_list}
    do
        echo "============ 生成 $dy db数据 ============"
        if [ "$dy" == "$start_date" ];
        then
            flag=`cat ${INSTALL_PATH}/dataware/db/application.properties |grep mock.clear.user |awk -F '=' '{print $2}'`
            if [ "x$flag" == "x0" ];
            then
                sed -i "s@^mock.clear=.*@mock.clear=1@g" ${INSTALL_PATH}/dataware/db/application.properties
                sed -i "s@^mock.clear.user=.*@mock.clear.user=1@g" ${INSTALL_PATH}/dataware/db/application.properties
            fi
        else
            sed -i "s@^mock.clear=.*@mock.clear=0@g" ${INSTALL_PATH}/dataware/db/application.properties
            sed -i "s@^mock.clear.user=.*@mock.clear.user=0@g" ${INSTALL_PATH}/dataware/db/application.properties
        fi

        dy_in=`date -d "$dy" +"%Y-%m-%d"`
        sed -i "s@^mock.date=.*@mock.date=$dy_in@g" ${INSTALL_PATH}/dataware/db/application.properties
        ssh hdp101 "source /etc/profile; cd ${INSTALL_PATH}/dataware/db; java -jar gmall2020-mock-db-2021-11-14.jar >/dev/null 2>&1 &"
        sleep 20s
    done

}

data_mock(){
    usage="Usage: $0 (date(eg:2021-06-14))"

    if [ $# -lt 1 ]; then
        echo $usage
        exit 1
    fi
    date=$1
    echo "============ 生成 $date log 数据 ============"
    sed -i "s@^mock.date:.*@mock.date: \"$date\"@g" ${INSTALL_PATH}/dataware/log/application.yml
    ssh hdp101 "source /etc/profile; cd ${INSTALL_PATH}/dataware/log; java -jar gmall2020-mock-log-2021-10-10.jar >/dev/null 2>&1 &"
    echo "============ 生成 $date 业务数据 ============"
    sed -i "s@^mock.date=.*@mock.date=$date@g" ${INSTALL_PATH}/dataware/db/application.properties
    ssh hdp101 "source /etc/profile; cd ${INSTALL_PATH}/dataware/db; java -jar gmall2020-mock-db-2021-11-14.jar >/dev/null 2>&1 &"
}

hadoop_init(){
    echo "--------hadoop format-------"
    bigstart dfs format
    echo "--------start hadoop cluster-------"
    bigstart dfs start
    echo "--------create hadoop directory and upload spark3.0.0 jars-------"
    hadoop fs -mkdir /spark-jars
    hadoop fs -mkdir /spark-log
    wget -P ${INSTALL_PATH}/ https://mirrors.huaweicloud.com/apache/spark/spark-3.0.0/spark-3.0.0-bin-without-hadoop.tgz
    tar -zxvf ${INSTALL_PATH}/spark-3.0.0-bin-without-hadoop.tgz -C ${INSTALL_PATH}
    hadoop fs -put ${INSTALL_PATH}/spark-3.0.0-bin-without-hadoop/jars/* /spark-jars
    rm -rf ${INSTALL_PATH}/spark-3.0.0-bin-without-hadoop*
    echo "--------init hive-------"
    ${INSTALL_PATH}/hive/bin/schematool -initSchema -dbType mysql

}

hive_init(){
    echo "--------hive init-------"
    ${INSTALL_PATH}/hive/bin/hive -f ${INSTALL_PATH}/dataware/sql/ods.sql
    #hadoop fs -mkdir -p /user/hive/jars
    #hadoop fs -put /opt/module/hive/lib/hivefunction-1.0-SNAPSHOT.jar /user/hive/jars
    #${INSTALL_PATH}/hive/bin/hive -e "create function gmall.explode_json_array as 'com.atguigu.hive.udtf.ExplodeJSONArray' using jar 'hdfs://${HOSTNAME_LIST[0]}:8020/user/hive/jars/hivefunction-1.0-SNAPSHOT.jar';"
}

mysql_init(){
    echo "--------mysql init-------"
    ssh hdp103 "source /etc/profile; mysql -uroot -p000000 gmall < ${INSTALL_PATH}/dataware/sql/gmall.sql"
    ssh hdp103 "source /etc/profile;mysql -uroot -p000000 gmall_report < ${INSTALL_PATH}/dataware/sql/gmall_report.sql"
}

init()
{
    # 启动 Zookeeper集群
    bigstart zookeeper start
    # 初始化hadoop和hive，启动hadoop
    hadoop_init
    #bigstart kafka start
    # 启动 Kafka采集集群
    bigstart kafka start
    # 启动采集 Flume
    f1 start
    # 启动日志消费 Flume
    f2 start

    mysql_init
    lg_init 20200614
    db_init 20200610 20200614

    # 清除maxwell断点痕迹
    # ssh hdp103 "source /etc/profile; mysql -uroot -p000000 -e 'drop table maxwell.bootstrap;drop table maxwell.columns;drop table maxwell.databases;drop table maxwell.heartbeats;drop table maxwell.positions;drop table maxwell.schemas;drop table maxwell.tables'"
    sed -i "6amock_date=2020-06-14" ${INSTALL_PATH}/maxwell/config.properties
    # 启动 maxwell
    bigstart maxwell start
    gen_import_config.sh
    # 启动业务消费 Flume
    f3 start
    # 全量数据表同步
    mysql_to_hdfs_full.sh all 2020-06-14

    mysql_to_kafka_inc_init.sh all

}

cluster() {
    case $1 in
        start)
        echo ================== 启动 集群 ==================

        # 启动 Zookeeper集群
        bigstart zookeeper start

        # 启动 Hadoop集群
        bigstart dfs start

        # 启动 Kafka采集集群
        bigstart kafka start

        # 启动采集 Flume
        f1 start

        # 启动日志消费 Flume
        f2 start

        # 启动业务消费 Flume
        f3 start

        # 启动 maxwell
        bigstart maxwell start

        ;;
    stop)
        echo ================== 停止 集群 ==================

        # 停止 Maxwell
        bigstart maxwell stop

        # 停止 业务消费Flume
        f3 stop

        # 停止 日志消费Flume
        f2 stop

        # 停止 日志采集Flume
        f1 stop

        # 停止 Kafka采集集群
        bigstart kafka stop

        # 停止 Hadoop集群
        bigstart dfs stop

        # 停止 Zookeeper集群
        bigstart zookeeper stop

    ;;
    esac
}

args()
{
    usage="Usage: $0 (f1|f2|f3|lg|db|db_init|log_init|cluster)"

    if [ $# -lt 1 ]; then
        echo $usage
        exit 1
    fi

    case $1 in
	  f1)
		f1 $2
		;;
	  f2)
		f2 $2
		;;
	  f3)
		f3 $2
		;;
	  lg)
		lg $2
		;;
	  db)
		db $2
		;;
	  lg_init)
		lg_init $2
		;;
	  db_init)
		db_init $2 $3
		;;
	  init)
		init
		;;
	  mock)
		data_mock $2
		;;
	  cluster)
		cluster $2
		;;
	  *)
		echo $usage
		;;
    esac
}
args $@
