# -----------------------------------2.7.7----------------------------------
# 用于指定NameNode的地址
# fs.defaultFS=hdfs://hdp101:9000
# Hadoop运行时产生文件的临时存储目录
# hadoop.tmp.dir=/home/vagrant/apps/hadoop/tmp
# 文件被永久删除前保留的时间（单位：分钟），默认值为0表明垃圾回收站功能关闭
# fs.trash.interval=10080
# fs.trash.checkpoint.interval=0

# 配置HDFS网页登录使用的静态用户为vagrant
# hadoop.http.staticuser.user=vagrant
# 配置该vagrant(superUser)允许通过代理访问的主机节点
# hadoop.proxyuser.vagrant.hosts=*
# 配置该vagrant(superUser)允许通过代理用户所属组
# hadoop.proxyuser.vagrant.groups=*
# 配置该vagrant(superUser)允许通过代理的用户
# hadoop.proxyuser.vagrant.users=*

#指定zookeeper地址，配置HA时需要
# ha.zookeeper.quorum=hdp101:2181,hdp102:2181,hdp103:2181

# -----------------------------------3.1.3-----------------------------------
# 用于指定NameNode的地址
fs.defaultFS=hdfs://hdp101:8020
# Hadoop运行时产生文件的临时存储目录
hadoop.tmp.dir=/home/vagrant/apps/hadoop/tmp
# 不开启权限检查
dfs.permissions.enabled=false
# 配置HDFS网页登录使用的静态用户为root
hadoop.http.staticuser.user=root
# 配置该vagrant(superUser)允许通过代理访问的主机节点
hadoop.proxyuser.root.hosts=*
# 配置该vagrant(superUser)允许通过代理用户所属组
hadoop.proxyuser.root.groups=*
# 配置该vagrant(superUser)允许通过代理的用户
hadoop.proxyuser.root.users=*
# 压缩配置
io.compression.codecs=org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.SnappyCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec
io.compression.codec.lzo.class=com.hadoop.compression.lzo.LzoCodec
