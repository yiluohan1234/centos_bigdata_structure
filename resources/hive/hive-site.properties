hive.metastore.local=false
system:java.io.tmpdir=/home/vagrant/apps/hive/tmpdir
javax.jdo.option.ConnectionURL=jdbc:mysql://hdp103:3306/hive?createDatabaseIfNotExist=true&amp;useSSL=false
javax.jdo.option.ConnectionDriverName=com.mysql.jdbc.Driver
javax.jdo.option.ConnectionUserName=hive
javax.jdo.option.ConnectionPassword=hive
hive.metastore.warehouse.dir=/user/hive/warehouse
datanucleus.autoCreateSchema=false
hive.metastore.schema.verification=false
hive.cli.print.current.db=true
hive.cli.print.header=true
hive.server2.thrift.port=10000
hive.server2.thrift.bind.host=hdp101
hive.metastore.event.db.notification.api.auth=false
# Spark依赖位置（注意：端口号8020必须和namenode的端口号一致）
# wget https://mirrors.huaweicloud.com/apache/spark/spark-3.0.0/spark-3.0.0-bin-without-hadoop.tgz
# spark.yarn.jars=hdfs://hdp101:8020/spark-jars/*
# hive.execution.engine=spark

# hive.server2.authentication=kerberos
# hive.server2.authentication.kerberos.principal=hive/hdp101@EXAMPLE.COM
# hive.server2.authentication.kerberos.keytab=/etc/security/keytab/hive.service.keytab
# hive.metastore.sasl.enabled=true
# hive.metastore.kerberos.keytab.file=/etc/security/keytab/hive.service.keytab
# hive.metastore.kerberos.principal=hive/hdp101@EXAMPLE.COM
# hive.exec.post.hooks=org.apache.atlas.hive.hook.HiveHook